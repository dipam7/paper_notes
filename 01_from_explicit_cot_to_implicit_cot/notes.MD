# Notes

Link to paper: https://arxiv.org/pdf/2405.14838

The paper talks about starting with CoT and then having the model internalize it. It uses the example of multiplying numbers which LLMs are traditionally bad at.
The first thoughts that come to mind when reading this paper are

1) Simple idea (which I love and am a fan of)
2) Mimics how humans learn? (as an intern I had to be told things but slowly they became intuition / second nature)

Even tho this method works well on certain tasks, further research is needed to explore its efficacy across a broader range of tasks and more diverse CoT traces.

TODOs:
- [ ] Check GSM8K dataset
- [ ] Checkout papers [6], [14] and [19] from references
- [ ] Did not understand this "The intermediate steps are also reversed to make it easier for the model to predict" (see paper 17 for more)
